{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "In this notebook, we will implement an RL algorithm with policy gradient to play Cartpole.\n",
    "\n",
    "Code is from https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Policy%20Gradients/Cartpole/Cartpole%20REINFORCE%20Monte%20Carlo%20Policy%20Gradients.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "# Policy gradient has high variance, seed for reproducability\n",
    "env.seed(1)\n",
    "\n",
    "## ENVIRONMENT Hyperparameters\n",
    "state_size = 4\n",
    "action_size = env.action_space.n\n",
    "\n",
    "## TRAINING Hyperparameters\n",
    "max_episodes = 300\n",
    "learning_rate = 0.01\n",
    "gamma = 0.95 # Discount rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.6860966e-01, 1.4645028e+38, 8.6090848e-02, 3.0545910e+37],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07863874,  0.83349357,  0.34320324, -0.63737744, -1.61795811])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    \n",
    "    return discounted_episode_rewards\n",
    "\n",
    "discount_and_normalize_rewards([2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  0\n",
      "Reward:  20.0\n",
      "Mean Reward 20.0\n",
      "Max reward so far:  20.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  1\n",
      "Reward:  35.0\n",
      "Mean Reward 27.5\n",
      "Max reward so far:  35.0\n",
      "==========================================\n",
      "Episode:  2\n",
      "Reward:  21.0\n",
      "Mean Reward 25.333333333333332\n",
      "Max reward so far:  35.0\n",
      "==========================================\n",
      "Episode:  3\n",
      "Reward:  31.0\n",
      "Mean Reward 26.75\n",
      "Max reward so far:  35.0\n",
      "==========================================\n",
      "Episode:  4\n",
      "Reward:  25.0\n",
      "Mean Reward 26.4\n",
      "Max reward so far:  35.0\n",
      "==========================================\n",
      "Episode:  5\n",
      "Reward:  27.0\n",
      "Mean Reward 26.5\n",
      "Max reward so far:  35.0\n",
      "==========================================\n",
      "Episode:  6\n",
      "Reward:  25.0\n",
      "Mean Reward 26.285714285714285\n",
      "Max reward so far:  35.0\n",
      "==========================================\n",
      "Episode:  7\n",
      "Reward:  20.0\n",
      "Mean Reward 25.5\n",
      "Max reward so far:  35.0\n",
      "==========================================\n",
      "Episode:  8\n",
      "Reward:  15.0\n",
      "Mean Reward 24.333333333333332\n",
      "Max reward so far:  35.0\n",
      "==========================================\n",
      "Episode:  9\n",
      "Reward:  27.0\n",
      "Mean Reward 24.6\n",
      "Max reward so far:  35.0\n",
      "==========================================\n",
      "Episode:  10\n",
      "Reward:  34.0\n",
      "Mean Reward 25.454545454545453\n",
      "Max reward so far:  35.0\n",
      "==========================================\n",
      "Episode:  11\n",
      "Reward:  13.0\n",
      "Mean Reward 24.416666666666668\n",
      "Max reward so far:  35.0\n",
      "==========================================\n",
      "Episode:  12\n",
      "Reward:  16.0\n",
      "Mean Reward 23.76923076923077\n",
      "Max reward so far:  35.0\n",
      "==========================================\n",
      "Episode:  13\n",
      "Reward:  15.0\n",
      "Mean Reward 23.142857142857142\n",
      "Max reward so far:  35.0\n",
      "==========================================\n",
      "Episode:  14\n",
      "Reward:  14.0\n",
      "Mean Reward 22.533333333333335\n",
      "Max reward so far:  35.0\n",
      "==========================================\n",
      "Episode:  15\n",
      "Reward:  42.0\n",
      "Mean Reward 23.75\n",
      "Max reward so far:  42.0\n",
      "==========================================\n",
      "Episode:  16\n",
      "Reward:  51.0\n",
      "Mean Reward 25.352941176470587\n",
      "Max reward so far:  51.0\n",
      "==========================================\n",
      "Episode:  17\n",
      "Reward:  17.0\n",
      "Mean Reward 24.88888888888889\n",
      "Max reward so far:  51.0\n",
      "==========================================\n",
      "Episode:  18\n",
      "Reward:  26.0\n",
      "Mean Reward 24.94736842105263\n",
      "Max reward so far:  51.0\n",
      "==========================================\n",
      "Episode:  19\n",
      "Reward:  25.0\n",
      "Mean Reward 24.95\n",
      "Max reward so far:  51.0\n",
      "==========================================\n",
      "Episode:  20\n",
      "Reward:  26.0\n",
      "Mean Reward 25.0\n",
      "Max reward so far:  51.0\n",
      "==========================================\n",
      "Episode:  21\n",
      "Reward:  20.0\n",
      "Mean Reward 24.772727272727273\n",
      "Max reward so far:  51.0\n",
      "==========================================\n",
      "Episode:  22\n",
      "Reward:  30.0\n",
      "Mean Reward 25.0\n",
      "Max reward so far:  51.0\n",
      "==========================================\n",
      "Episode:  23\n",
      "Reward:  21.0\n",
      "Mean Reward 24.833333333333332\n",
      "Max reward so far:  51.0\n",
      "==========================================\n",
      "Episode:  24\n",
      "Reward:  56.0\n",
      "Mean Reward 26.08\n",
      "Max reward so far:  56.0\n",
      "==========================================\n",
      "Episode:  25\n",
      "Reward:  14.0\n",
      "Mean Reward 25.615384615384617\n",
      "Max reward so far:  56.0\n",
      "==========================================\n",
      "Episode:  26\n",
      "Reward:  56.0\n",
      "Mean Reward 26.74074074074074\n",
      "Max reward so far:  56.0\n",
      "==========================================\n",
      "Episode:  27\n",
      "Reward:  50.0\n",
      "Mean Reward 27.571428571428573\n",
      "Max reward so far:  56.0\n",
      "==========================================\n",
      "Episode:  28\n",
      "Reward:  44.0\n",
      "Mean Reward 28.137931034482758\n",
      "Max reward so far:  56.0\n",
      "==========================================\n",
      "Episode:  29\n",
      "Reward:  71.0\n",
      "Mean Reward 29.566666666666666\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  30\n",
      "Reward:  22.0\n",
      "Mean Reward 29.322580645161292\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  31\n",
      "Reward:  67.0\n",
      "Mean Reward 30.5\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  32\n",
      "Reward:  29.0\n",
      "Mean Reward 30.454545454545453\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  33\n",
      "Reward:  21.0\n",
      "Mean Reward 30.176470588235293\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  34\n",
      "Reward:  39.0\n",
      "Mean Reward 30.428571428571427\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  35\n",
      "Reward:  25.0\n",
      "Mean Reward 30.27777777777778\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  36\n",
      "Reward:  45.0\n",
      "Mean Reward 30.675675675675677\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  37\n",
      "Reward:  23.0\n",
      "Mean Reward 30.473684210526315\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  38\n",
      "Reward:  27.0\n",
      "Mean Reward 30.384615384615383\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  39\n",
      "Reward:  26.0\n",
      "Mean Reward 30.275\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  40\n",
      "Reward:  64.0\n",
      "Mean Reward 31.097560975609756\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  41\n",
      "Reward:  14.0\n",
      "Mean Reward 30.69047619047619\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  42\n",
      "Reward:  16.0\n",
      "Mean Reward 30.348837209302324\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  43\n",
      "Reward:  17.0\n",
      "Mean Reward 30.045454545454547\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  44\n",
      "Reward:  16.0\n",
      "Mean Reward 29.733333333333334\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  45\n",
      "Reward:  19.0\n",
      "Mean Reward 29.5\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  46\n",
      "Reward:  36.0\n",
      "Mean Reward 29.638297872340427\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  47\n",
      "Reward:  54.0\n",
      "Mean Reward 30.145833333333332\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  48\n",
      "Reward:  37.0\n",
      "Mean Reward 30.285714285714285\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  49\n",
      "Reward:  23.0\n",
      "Mean Reward 30.14\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  50\n",
      "Reward:  13.0\n",
      "Mean Reward 29.80392156862745\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  51\n",
      "Reward:  50.0\n",
      "Mean Reward 30.192307692307693\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  52\n",
      "Reward:  10.0\n",
      "Mean Reward 29.81132075471698\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  53\n",
      "Reward:  37.0\n",
      "Mean Reward 29.944444444444443\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  54\n",
      "Reward:  18.0\n",
      "Mean Reward 29.727272727272727\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  55\n",
      "Reward:  31.0\n",
      "Mean Reward 29.75\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  56\n",
      "Reward:  14.0\n",
      "Mean Reward 29.473684210526315\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  57\n",
      "Reward:  10.0\n",
      "Mean Reward 29.137931034482758\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  58\n",
      "Reward:  22.0\n",
      "Mean Reward 29.016949152542374\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  59\n",
      "Reward:  38.0\n",
      "Mean Reward 29.166666666666668\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  60\n",
      "Reward:  44.0\n",
      "Mean Reward 29.40983606557377\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  61\n",
      "Reward:  32.0\n",
      "Mean Reward 29.451612903225808\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  62\n",
      "Reward:  16.0\n",
      "Mean Reward 29.238095238095237\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  63\n",
      "Reward:  11.0\n",
      "Mean Reward 28.953125\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  64\n",
      "Reward:  21.0\n",
      "Mean Reward 28.83076923076923\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  65\n",
      "Reward:  20.0\n",
      "Mean Reward 28.696969696969695\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  66\n",
      "Reward:  23.0\n",
      "Mean Reward 28.611940298507463\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  67\n",
      "Reward:  12.0\n",
      "Mean Reward 28.36764705882353\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  68\n",
      "Reward:  34.0\n",
      "Mean Reward 28.44927536231884\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  69\n",
      "Reward:  17.0\n",
      "Mean Reward 28.285714285714285\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  70\n",
      "Reward:  13.0\n",
      "Mean Reward 28.070422535211268\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  71\n",
      "Reward:  35.0\n",
      "Mean Reward 28.166666666666668\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  72\n",
      "Reward:  19.0\n",
      "Mean Reward 28.041095890410958\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  73\n",
      "Reward:  30.0\n",
      "Mean Reward 28.06756756756757\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  74\n",
      "Reward:  50.0\n",
      "Mean Reward 28.36\n",
      "Max reward so far:  71.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  75\n",
      "Reward:  26.0\n",
      "Mean Reward 28.32894736842105\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  76\n",
      "Reward:  18.0\n",
      "Mean Reward 28.194805194805195\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  77\n",
      "Reward:  12.0\n",
      "Mean Reward 27.987179487179485\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  78\n",
      "Reward:  21.0\n",
      "Mean Reward 27.89873417721519\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  79\n",
      "Reward:  19.0\n",
      "Mean Reward 27.7875\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  80\n",
      "Reward:  46.0\n",
      "Mean Reward 28.012345679012345\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  81\n",
      "Reward:  29.0\n",
      "Mean Reward 28.024390243902438\n",
      "Max reward so far:  71.0\n",
      "==========================================\n",
      "Episode:  82\n",
      "Reward:  77.0\n",
      "Mean Reward 28.6144578313253\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  83\n",
      "Reward:  14.0\n",
      "Mean Reward 28.44047619047619\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  84\n",
      "Reward:  27.0\n",
      "Mean Reward 28.423529411764704\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  85\n",
      "Reward:  88.0\n",
      "Mean Reward 29.11627906976744\n",
      "Max reward so far:  88.0\n",
      "==========================================\n",
      "Episode:  86\n",
      "Reward:  37.0\n",
      "Mean Reward 29.20689655172414\n",
      "Max reward so far:  88.0\n",
      "==========================================\n",
      "Episode:  87\n",
      "Reward:  59.0\n",
      "Mean Reward 29.545454545454547\n",
      "Max reward so far:  88.0\n",
      "==========================================\n",
      "Episode:  88\n",
      "Reward:  40.0\n",
      "Mean Reward 29.662921348314608\n",
      "Max reward so far:  88.0\n",
      "==========================================\n",
      "Episode:  89\n",
      "Reward:  61.0\n",
      "Mean Reward 30.011111111111113\n",
      "Max reward so far:  88.0\n",
      "==========================================\n",
      "Episode:  90\n",
      "Reward:  26.0\n",
      "Mean Reward 29.967032967032967\n",
      "Max reward so far:  88.0\n",
      "==========================================\n",
      "Episode:  91\n",
      "Reward:  36.0\n",
      "Mean Reward 30.032608695652176\n",
      "Max reward so far:  88.0\n",
      "==========================================\n",
      "Episode:  92\n",
      "Reward:  32.0\n",
      "Mean Reward 30.053763440860216\n",
      "Max reward so far:  88.0\n",
      "==========================================\n",
      "Episode:  93\n",
      "Reward:  26.0\n",
      "Mean Reward 30.01063829787234\n",
      "Max reward so far:  88.0\n",
      "==========================================\n",
      "Episode:  94\n",
      "Reward:  22.0\n",
      "Mean Reward 29.926315789473684\n",
      "Max reward so far:  88.0\n",
      "==========================================\n",
      "Episode:  95\n",
      "Reward:  96.0\n",
      "Mean Reward 30.614583333333332\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  96\n",
      "Reward:  18.0\n",
      "Mean Reward 30.484536082474225\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  97\n",
      "Reward:  66.0\n",
      "Mean Reward 30.846938775510203\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  98\n",
      "Reward:  18.0\n",
      "Mean Reward 30.717171717171716\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  99\n",
      "Reward:  33.0\n",
      "Mean Reward 30.74\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  100\n",
      "Reward:  27.0\n",
      "Mean Reward 30.702970297029704\n",
      "Max reward so far:  96.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  101\n",
      "Reward:  24.0\n",
      "Mean Reward 30.637254901960784\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  102\n",
      "Reward:  65.0\n",
      "Mean Reward 30.97087378640777\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  103\n",
      "Reward:  42.0\n",
      "Mean Reward 31.076923076923077\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  104\n",
      "Reward:  27.0\n",
      "Mean Reward 31.038095238095238\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  105\n",
      "Reward:  31.0\n",
      "Mean Reward 31.037735849056602\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  106\n",
      "Reward:  43.0\n",
      "Mean Reward 31.149532710280372\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  107\n",
      "Reward:  32.0\n",
      "Mean Reward 31.15740740740741\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  108\n",
      "Reward:  11.0\n",
      "Mean Reward 30.972477064220183\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  109\n",
      "Reward:  71.0\n",
      "Mean Reward 31.336363636363636\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  110\n",
      "Reward:  64.0\n",
      "Mean Reward 31.63063063063063\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  111\n",
      "Reward:  19.0\n",
      "Mean Reward 31.517857142857142\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  112\n",
      "Reward:  37.0\n",
      "Mean Reward 31.56637168141593\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  113\n",
      "Reward:  23.0\n",
      "Mean Reward 31.49122807017544\n",
      "Max reward so far:  96.0\n",
      "==========================================\n",
      "Episode:  114\n",
      "Reward:  154.0\n",
      "Mean Reward 32.55652173913043\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  115\n",
      "Reward:  32.0\n",
      "Mean Reward 32.55172413793103\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  116\n",
      "Reward:  104.0\n",
      "Mean Reward 33.162393162393165\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  117\n",
      "Reward:  83.0\n",
      "Mean Reward 33.58474576271186\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  118\n",
      "Reward:  68.0\n",
      "Mean Reward 33.87394957983193\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  119\n",
      "Reward:  39.0\n",
      "Mean Reward 33.916666666666664\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  120\n",
      "Reward:  46.0\n",
      "Mean Reward 34.01652892561984\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  121\n",
      "Reward:  67.0\n",
      "Mean Reward 34.28688524590164\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  122\n",
      "Reward:  33.0\n",
      "Mean Reward 34.27642276422764\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  123\n",
      "Reward:  26.0\n",
      "Mean Reward 34.20967741935484\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  124\n",
      "Reward:  28.0\n",
      "Mean Reward 34.16\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  125\n",
      "Reward:  43.0\n",
      "Mean Reward 34.23015873015873\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  126\n",
      "Reward:  45.0\n",
      "Mean Reward 34.31496062992126\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  127\n",
      "Reward:  17.0\n",
      "Mean Reward 34.1796875\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  128\n",
      "Reward:  38.0\n",
      "Mean Reward 34.2093023255814\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  129\n",
      "Reward:  22.0\n",
      "Mean Reward 34.11538461538461\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  130\n",
      "Reward:  48.0\n",
      "Mean Reward 34.221374045801525\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  131\n",
      "Reward:  86.0\n",
      "Mean Reward 34.61363636363637\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  132\n",
      "Reward:  21.0\n",
      "Mean Reward 34.51127819548872\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  133\n",
      "Reward:  38.0\n",
      "Mean Reward 34.53731343283582\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  134\n",
      "Reward:  65.0\n",
      "Mean Reward 34.762962962962966\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  135\n",
      "Reward:  55.0\n",
      "Mean Reward 34.911764705882355\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  136\n",
      "Reward:  29.0\n",
      "Mean Reward 34.86861313868613\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  137\n",
      "Reward:  54.0\n",
      "Mean Reward 35.007246376811594\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  138\n",
      "Reward:  63.0\n",
      "Mean Reward 35.20863309352518\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  139\n",
      "Reward:  125.0\n",
      "Mean Reward 35.85\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  140\n",
      "Reward:  19.0\n",
      "Mean Reward 35.730496453900706\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  141\n",
      "Reward:  51.0\n",
      "Mean Reward 35.83802816901409\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  142\n",
      "Reward:  128.0\n",
      "Mean Reward 36.48251748251748\n",
      "Max reward so far:  154.0\n",
      "==========================================\n",
      "Episode:  143\n",
      "Reward:  123.0\n",
      "Mean Reward 37.083333333333336\n",
      "Max reward so far:  154.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  144\n",
      "Reward:  200.0\n",
      "Mean Reward 38.206896551724135\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  145\n",
      "Reward:  12.0\n",
      "Mean Reward 38.02739726027397\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  146\n",
      "Reward:  78.0\n",
      "Mean Reward 38.29931972789116\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  147\n",
      "Reward:  31.0\n",
      "Mean Reward 38.25\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  148\n",
      "Reward:  23.0\n",
      "Mean Reward 38.147651006711406\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  149\n",
      "Reward:  55.0\n",
      "Mean Reward 38.26\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  150\n",
      "Reward:  69.0\n",
      "Mean Reward 38.4635761589404\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  151\n",
      "Reward:  78.0\n",
      "Mean Reward 38.723684210526315\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  152\n",
      "Reward:  38.0\n",
      "Mean Reward 38.71895424836601\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  153\n",
      "Reward:  50.0\n",
      "Mean Reward 38.79220779220779\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  154\n",
      "Reward:  89.0\n",
      "Mean Reward 39.116129032258065\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  155\n",
      "Reward:  124.0\n",
      "Mean Reward 39.66025641025641\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  156\n",
      "Reward:  90.0\n",
      "Mean Reward 39.98089171974522\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  157\n",
      "Reward:  17.0\n",
      "Mean Reward 39.835443037974684\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  158\n",
      "Reward:  32.0\n",
      "Mean Reward 39.78616352201258\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  159\n",
      "Reward:  73.0\n",
      "Mean Reward 39.99375\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  160\n",
      "Reward:  123.0\n",
      "Mean Reward 40.50931677018634\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  161\n",
      "Reward:  48.0\n",
      "Mean Reward 40.55555555555556\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  162\n",
      "Reward:  167.0\n",
      "Mean Reward 41.331288343558285\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  163\n",
      "Reward:  144.0\n",
      "Mean Reward 41.957317073170735\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  164\n",
      "Reward:  25.0\n",
      "Mean Reward 41.85454545454545\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  165\n",
      "Reward:  124.0\n",
      "Mean Reward 42.34939759036145\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  166\n",
      "Reward:  13.0\n",
      "Mean Reward 42.17365269461078\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  167\n",
      "Reward:  17.0\n",
      "Mean Reward 42.023809523809526\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  168\n",
      "Reward:  145.0\n",
      "Mean Reward 42.633136094674555\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  169\n",
      "Reward:  69.0\n",
      "Mean Reward 42.78823529411765\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  170\n",
      "Reward:  37.0\n",
      "Mean Reward 42.75438596491228\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  171\n",
      "Reward:  57.0\n",
      "Mean Reward 42.83720930232558\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  172\n",
      "Reward:  110.0\n",
      "Mean Reward 43.225433526011564\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  173\n",
      "Reward:  17.0\n",
      "Mean Reward 43.07471264367816\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  174\n",
      "Reward:  49.0\n",
      "Mean Reward 43.10857142857143\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  175\n",
      "Reward:  25.0\n",
      "Mean Reward 43.00568181818182\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  176\n",
      "Reward:  64.0\n",
      "Mean Reward 43.12429378531073\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  177\n",
      "Reward:  117.0\n",
      "Mean Reward 43.53932584269663\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  178\n",
      "Reward:  134.0\n",
      "Mean Reward 44.04469273743017\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  179\n",
      "Reward:  101.0\n",
      "Mean Reward 44.361111111111114\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  180\n",
      "Reward:  77.0\n",
      "Mean Reward 44.5414364640884\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  181\n",
      "Reward:  62.0\n",
      "Mean Reward 44.637362637362635\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  182\n",
      "Reward:  157.0\n",
      "Mean Reward 45.25136612021858\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  183\n",
      "Reward:  137.0\n",
      "Mean Reward 45.75\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  184\n",
      "Reward:  74.0\n",
      "Mean Reward 45.902702702702705\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  185\n",
      "Reward:  81.0\n",
      "Mean Reward 46.09139784946237\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  186\n",
      "Reward:  36.0\n",
      "Mean Reward 46.037433155080215\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  187\n",
      "Reward:  137.0\n",
      "Mean Reward 46.52127659574468\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  188\n",
      "Reward:  141.0\n",
      "Mean Reward 47.02116402116402\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  189\n",
      "Reward:  12.0\n",
      "Mean Reward 46.83684210526316\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  190\n",
      "Reward:  87.0\n",
      "Mean Reward 47.047120418848166\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  191\n",
      "Reward:  87.0\n",
      "Mean Reward 47.255208333333336\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  192\n",
      "Reward:  131.0\n",
      "Mean Reward 47.689119170984455\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  193\n",
      "Reward:  64.0\n",
      "Mean Reward 47.77319587628866\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  194\n",
      "Reward:  89.0\n",
      "Mean Reward 47.98461538461538\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  195\n",
      "Reward:  102.0\n",
      "Mean Reward 48.26020408163265\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  196\n",
      "Reward:  200.0\n",
      "Mean Reward 49.03045685279188\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  197\n",
      "Reward:  131.0\n",
      "Mean Reward 49.44444444444444\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  198\n",
      "Reward:  53.0\n",
      "Mean Reward 49.462311557788944\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  199\n",
      "Reward:  26.0\n",
      "Mean Reward 49.345\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  200\n",
      "Reward:  79.0\n",
      "Mean Reward 49.492537313432834\n",
      "Max reward so far:  200.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  201\n",
      "Reward:  70.0\n",
      "Mean Reward 49.59405940594059\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  202\n",
      "Reward:  157.0\n",
      "Mean Reward 50.12315270935961\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  203\n",
      "Reward:  133.0\n",
      "Mean Reward 50.529411764705884\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  204\n",
      "Reward:  86.0\n",
      "Mean Reward 50.702439024390245\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  205\n",
      "Reward:  44.0\n",
      "Mean Reward 50.66990291262136\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  206\n",
      "Reward:  64.0\n",
      "Mean Reward 50.734299516908216\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  207\n",
      "Reward:  122.0\n",
      "Mean Reward 51.07692307692308\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  208\n",
      "Reward:  86.0\n",
      "Mean Reward 51.24401913875598\n",
      "Max reward so far:  200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  209\n",
      "Reward:  161.0\n",
      "Mean Reward 51.766666666666666\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  210\n",
      "Reward:  13.0\n",
      "Mean Reward 51.58293838862559\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  211\n",
      "Reward:  88.0\n",
      "Mean Reward 51.75471698113208\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  212\n",
      "Reward:  47.0\n",
      "Mean Reward 51.732394366197184\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  213\n",
      "Reward:  190.0\n",
      "Mean Reward 52.37850467289719\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  214\n",
      "Reward:  140.0\n",
      "Mean Reward 52.78604651162791\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  215\n",
      "Reward:  161.0\n",
      "Mean Reward 53.28703703703704\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  216\n",
      "Reward:  55.0\n",
      "Mean Reward 53.294930875576036\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  217\n",
      "Reward:  176.0\n",
      "Mean Reward 53.857798165137616\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  218\n",
      "Reward:  164.0\n",
      "Mean Reward 54.36073059360731\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  219\n",
      "Reward:  200.0\n",
      "Mean Reward 55.02272727272727\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  220\n",
      "Reward:  137.0\n",
      "Mean Reward 55.39366515837104\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  221\n",
      "Reward:  161.0\n",
      "Mean Reward 55.869369369369366\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  222\n",
      "Reward:  138.0\n",
      "Mean Reward 56.237668161434975\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  223\n",
      "Reward:  101.0\n",
      "Mean Reward 56.4375\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  224\n",
      "Reward:  192.0\n",
      "Mean Reward 57.04\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  225\n",
      "Reward:  23.0\n",
      "Mean Reward 56.889380530973455\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  226\n",
      "Reward:  200.0\n",
      "Mean Reward 57.519823788546255\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  227\n",
      "Reward:  200.0\n",
      "Mean Reward 58.14473684210526\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  228\n",
      "Reward:  200.0\n",
      "Mean Reward 58.76419213973799\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  229\n",
      "Reward:  127.0\n",
      "Mean Reward 59.06086956521739\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  230\n",
      "Reward:  79.0\n",
      "Mean Reward 59.14718614718615\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  231\n",
      "Reward:  191.0\n",
      "Mean Reward 59.71551724137931\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  232\n",
      "Reward:  200.0\n",
      "Mean Reward 60.317596566523605\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  233\n",
      "Reward:  166.0\n",
      "Mean Reward 60.76923076923077\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  234\n",
      "Reward:  200.0\n",
      "Mean Reward 61.361702127659576\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  235\n",
      "Reward:  200.0\n",
      "Mean Reward 61.94915254237288\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  236\n",
      "Reward:  109.0\n",
      "Mean Reward 62.14767932489451\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  237\n",
      "Reward:  195.0\n",
      "Mean Reward 62.705882352941174\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  238\n",
      "Reward:  200.0\n",
      "Mean Reward 63.28033472803347\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  239\n",
      "Reward:  129.0\n",
      "Mean Reward 63.55416666666667\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  240\n",
      "Reward:  200.0\n",
      "Mean Reward 64.12033195020747\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  241\n",
      "Reward:  154.0\n",
      "Mean Reward 64.49173553719008\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  242\n",
      "Reward:  194.0\n",
      "Mean Reward 65.0246913580247\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  243\n",
      "Reward:  181.0\n",
      "Mean Reward 65.5\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  244\n",
      "Reward:  60.0\n",
      "Mean Reward 65.47755102040816\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  245\n",
      "Reward:  177.0\n",
      "Mean Reward 65.9308943089431\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  246\n",
      "Reward:  154.0\n",
      "Mean Reward 66.28744939271255\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  247\n",
      "Reward:  163.0\n",
      "Mean Reward 66.6774193548387\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  248\n",
      "Reward:  159.0\n",
      "Mean Reward 67.04819277108433\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  249\n",
      "Reward:  190.0\n",
      "Mean Reward 67.54\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  250\n",
      "Reward:  117.0\n",
      "Mean Reward 67.73705179282868\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  251\n",
      "Reward:  148.0\n",
      "Mean Reward 68.05555555555556\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  252\n",
      "Reward:  146.0\n",
      "Mean Reward 68.36363636363636\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  253\n",
      "Reward:  189.0\n",
      "Mean Reward 68.83858267716535\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  254\n",
      "Reward:  157.0\n",
      "Mean Reward 69.1843137254902\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  255\n",
      "Reward:  94.0\n",
      "Mean Reward 69.28125\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  256\n",
      "Reward:  181.0\n",
      "Mean Reward 69.715953307393\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  257\n",
      "Reward:  152.0\n",
      "Mean Reward 70.03488372093024\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  258\n",
      "Reward:  144.0\n",
      "Mean Reward 70.32046332046332\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  259\n",
      "Reward:  131.0\n",
      "Mean Reward 70.55384615384615\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  260\n",
      "Reward:  126.0\n",
      "Mean Reward 70.76628352490421\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  261\n",
      "Reward:  151.0\n",
      "Mean Reward 71.07251908396947\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  262\n",
      "Reward:  157.0\n",
      "Mean Reward 71.39923954372624\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  263\n",
      "Reward:  152.0\n",
      "Mean Reward 71.70454545454545\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  264\n",
      "Reward:  175.0\n",
      "Mean Reward 72.09433962264151\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  265\n",
      "Reward:  144.0\n",
      "Mean Reward 72.36466165413533\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  266\n",
      "Reward:  44.0\n",
      "Mean Reward 72.25842696629213\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  267\n",
      "Reward:  187.0\n",
      "Mean Reward 72.68656716417911\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  268\n",
      "Reward:  200.0\n",
      "Mean Reward 73.15985130111524\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  269\n",
      "Reward:  200.0\n",
      "Mean Reward 73.62962962962963\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  270\n",
      "Reward:  200.0\n",
      "Mean Reward 74.09594095940959\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  271\n",
      "Reward:  200.0\n",
      "Mean Reward 74.55882352941177\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  272\n",
      "Reward:  200.0\n",
      "Mean Reward 75.01831501831502\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  273\n",
      "Reward:  200.0\n",
      "Mean Reward 75.47445255474453\n",
      "Max reward so far:  200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  274\n",
      "Reward:  177.0\n",
      "Mean Reward 75.84363636363636\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  275\n",
      "Reward:  200.0\n",
      "Mean Reward 76.29347826086956\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  276\n",
      "Reward:  200.0\n",
      "Mean Reward 76.74007220216606\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  277\n",
      "Reward:  200.0\n",
      "Mean Reward 77.18345323741008\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  278\n",
      "Reward:  200.0\n",
      "Mean Reward 77.6236559139785\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  279\n",
      "Reward:  200.0\n",
      "Mean Reward 78.06071428571428\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  280\n",
      "Reward:  200.0\n",
      "Mean Reward 78.49466192170819\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  281\n",
      "Reward:  194.0\n",
      "Mean Reward 78.90425531914893\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  282\n",
      "Reward:  200.0\n",
      "Mean Reward 79.3321554770318\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  283\n",
      "Reward:  200.0\n",
      "Mean Reward 79.75704225352112\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  284\n",
      "Reward:  180.0\n",
      "Mean Reward 80.10877192982456\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  285\n",
      "Reward:  200.0\n",
      "Mean Reward 80.52797202797203\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  286\n",
      "Reward:  99.0\n",
      "Mean Reward 80.59233449477352\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  287\n",
      "Reward:  200.0\n",
      "Mean Reward 81.00694444444444\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  288\n",
      "Reward:  200.0\n",
      "Mean Reward 81.41868512110727\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  289\n",
      "Reward:  200.0\n",
      "Mean Reward 81.82758620689656\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  290\n",
      "Reward:  200.0\n",
      "Mean Reward 82.23367697594502\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  291\n",
      "Reward:  200.0\n",
      "Mean Reward 82.63698630136986\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  292\n",
      "Reward:  200.0\n",
      "Mean Reward 83.03754266211604\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  293\n",
      "Reward:  200.0\n",
      "Mean Reward 83.43537414965986\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  294\n",
      "Reward:  200.0\n",
      "Mean Reward 83.83050847457628\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  295\n",
      "Reward:  200.0\n",
      "Mean Reward 84.22297297297297\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  296\n",
      "Reward:  200.0\n",
      "Mean Reward 84.61279461279462\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  297\n",
      "Reward:  56.0\n",
      "Mean Reward 84.51677852348993\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  298\n",
      "Reward:  151.0\n",
      "Mean Reward 84.73913043478261\n",
      "Max reward so far:  200.0\n",
      "==========================================\n",
      "Episode:  299\n",
      "Reward:  200.0\n",
      "Mean Reward 85.12333333333333\n",
      "Max reward so far:  200.0\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"inputs\"):\n",
    "    input_ = tf.placeholder(tf.float32, [None, state_size], name=\"input_\")\n",
    "    actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "    discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards\")\n",
    "    \n",
    "    # Add this placeholder for having this variable in tensorboard\n",
    "    mean_reward_ = tf.placeholder(tf.float32 , name=\"mean_reward\")\n",
    "\n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        fc1 = tf.contrib.layers.fully_connected(inputs = input_,\n",
    "                                                num_outputs = 10,\n",
    "                                                activation_fn=tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.name_scope(\"fc2\"):\n",
    "        fc2 = tf.contrib.layers.fully_connected(inputs = fc1,\n",
    "                                                num_outputs = action_size,\n",
    "                                                activation_fn= tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    with tf.name_scope(\"fc3\"):\n",
    "        fc3 = tf.contrib.layers.fully_connected(inputs = fc2,\n",
    "                                                num_outputs = action_size,\n",
    "                                                activation_fn= None,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        action_distribution = tf.nn.softmax(fc3)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "        # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = fc3, labels = actions)\n",
    "        loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards_) \n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        \n",
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\".\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "## Reward mean\n",
    "tf.summary.scalar(\"Reward_mean\", mean_reward_)\n",
    "\n",
    "write_op = tf.summary.merge_all()\n",
    "\n",
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        episode_rewards_sum = 0\n",
    "\n",
    "        # Launch the game\n",
    "        state = env.reset()\n",
    "           \n",
    "        while True:\n",
    "            \n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1,4])})\n",
    "            \n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel()) # select action w.r.t the actions prob\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Perform a\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Store s, a, r\n",
    "            episode_states.append(state)\n",
    "                        \n",
    "            # For actions because we output only one (the index) we need 2 (1 is for the action taken)\n",
    "            # We need [0., 1.] (if we take right) not just the index\n",
    "            action_ = np.zeros(action_size)\n",
    "            action_[action] = 1\n",
    "            \n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            if done:\n",
    "                # Calculate sum reward\n",
    "                episode_rewards_sum = np.sum(episode_rewards)\n",
    "                \n",
    "                allRewards.append(episode_rewards_sum)\n",
    "                \n",
    "                total_rewards = np.sum(allRewards)\n",
    "                \n",
    "                # Mean reward\n",
    "                mean_reward = np.divide(total_rewards, episode+1)\n",
    "                \n",
    "                \n",
    "                maximumRewardRecorded = np.amax(allRewards)\n",
    "                \n",
    "                print(\"==========================================\")\n",
    "                print(\"Episode: \", episode)\n",
    "                print(\"Reward: \", episode_rewards_sum)\n",
    "                print(\"Mean Reward\", mean_reward)\n",
    "                print(\"Max reward so far: \", maximumRewardRecorded)\n",
    "                \n",
    "                # Calculate discounted reward\n",
    "                discounted_episode_rewards = discount_and_normalize_rewards(episode_rewards)\n",
    "                                \n",
    "                # Feedforward, gradient and backpropagation\n",
    "                loss_, _ = sess.run([loss, train_opt], feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                                                                 discounted_episode_rewards_: discounted_episode_rewards \n",
    "                                                                })\n",
    "                \n",
    " \n",
    "                                                                 \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                                                                 discounted_episode_rewards_: discounted_episode_rewards,\n",
    "                                                                    mean_reward_: mean_reward\n",
    "                                                                })\n",
    "                \n",
    "               \n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "            \n",
    "                \n",
    "                # Reset the transition stores\n",
    "                episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "                \n",
    "                break\n",
    "            \n",
    "            state = new_state\n",
    "        \n",
    "        # Save Model\n",
    "        if episode % 100 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
